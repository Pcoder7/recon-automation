name: Parallel Subdomain Enumeration and Live Host Check

on:
  schedule:
    - cron: "0 10 * * *"  # Runs every day at 10:00 AM UTC
  workflow_dispatch:

permissions:
  contents: write

env:
  PDCP_KEY: ${{ secrets.PDCP_API_KEY }}  

jobs:
  generate_matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set_matrix.outputs.matrix }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3
        with:
          # Fetch full Git history so previous commits are available for comparison
          fetch-depth: 0        

      - name: Generate Matrix from domains.txt
        id: set_matrix
        run: |
          if [ ! -f domains.txt ]; then
            echo "domains.txt file not found!"
            exit 0
          fi
          # Create a JSON array from non-empty lines in domains.txt
          matrix=$(jq -R -s -c 'split("\n") | map(select(length > 0))' domains.txt)
          echo "Matrix: $matrix"
          echo "matrix=$matrix" >> $GITHUB_OUTPUT
          
  subdomain_scan:
    needs: generate_matrix
    runs-on: ubuntu-latest
    container:
      # Use the exact same container for consistency with the primary account
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}      
    strategy:
      fail-fast: false
      matrix:
        domain: ${{ fromJson(needs.generate_matrix.outputs.matrix) }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3
        with:
          # Fetch full Git history so previous commits are available for comparison
          fetch-depth: 0   
          
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-
         
      
      - name: Install Tools
        run: |
          # Installing Jq

          if ! command -v jq >/dev/null; then
            echo "Installing jq…"
            apt install jq
          else
            echo "Jq already installed"
          fi   
          # Installing smap
          if ! command -v smap >/dev/null; then
            echo "Installing smap…"
            go install -v github.com/s0md3v/smap/cmd/smap@latest
          else
            echo "smap already installed"
          fi    

          # Installing inscope
          if ! command -v inscope >/dev/null; then
            echo "Installing inscope…"
            go install -v github.com/tomnomnom/hacks/inscope@latest
          else
            echo "inscope already in installed"
          fi    
          
          # Installing Chaos-client
          if ! command -v chaos >/dev/null; then
            echo "Installing Chaos…"
            go install -v github.com/projectdiscovery/chaos-client/cmd/chaos@latest
          else
            echo "Chaos already in installed"
          fi    
          
          
          if ! command -v anew >/dev/null; then
            echo "Installing anew…"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in installed"
          fi

          if ! command -v shrewdeye >/dev/null; then
            echo "Installing Shrewdeye…"
            go install -v github.com/omkar7505/shrewdeye@latest
          else
            echo "shrewdeye already in installed"
          fi
          
          pip3 install --no-cache-dir ipaddress
          
          echo "$HOME/go/bin" >> $GITHUB_PATH
          
  
      - name: Install interlace tool
        shell: bash
        run: |
          python3 --version
          git clone https://github.com/codingo/Interlace.git
          cd Interlace
          python3 -m pip install -r requirements.txt
          pip3 install .
          python3 Interlace/interlace.py -h
          cd ..

      - name: Run Subdomain Enumeration for ${{ matrix.domain }}
        shell: bash
        run: |
          TMP_ONEFOR="$(mktemp)"
          OUTDIR="results/${{ matrix.domain }}"
          JSONFILE="${OUTDIR}/oneforall.json"
          ONEFORALL_TXT="${OUTDIR}/oneforall.txt"

          BBOT_OUTDIR="results/${{ matrix.domain }}/bbot-output"
          BBOT_TXT="${OUTDIR}/bbot.txt"
          EXTERNAL_TX="$(mktemp)"
          EXTERNAL_FINAL="${OUTDIR}/external.txt"
          TARGET="${OUTDIR}/all_subdomains.txt"

          echo "Scanning domain: ${{ matrix.domain }}"

          # run interlace (cli.txt drives the tools)
          interlace \
            -t ${{ matrix.domain }} \
            -threads 6 \
            -cL cli.txt \
            -o "$OUTDIR" \
            -v

          assetfinder -subs-only ${{ matrix.domain }} | sed -E "s#^https?://##I; s/^[*.]+//; s/[.]+$//; s/^[[:space:]]+//; s/[[:space:]]+$//" |  grep -E '^[a-z0-9]([a-z0-9-]*\.)+[a-z]{2,}$' --color=never | inscope --scope .scope | anew -q "$OUTDIR/assetfinder.txt" || true

          echo "Debug: check PDCP_KEY is present (length only)"
          if [ -z "${PDCP_KEY:-}" ]; then
            echo "PDCP_KEY is MISSING in this environment"
          else
            # print length only (safe)
            echo "PDCP_KEY present, length=$(printf '%s' "$PDCP_KEY" | wc -c)"
          fi

          chaos -d "${{ matrix.domain }}" -o "$OUTDIR/chaos.txt" -key "${PDCP_KEY}" -silent || true
         
          
          # --- oneforall: extract subdomain entries (fixed jq usage) ---
          if [[ -s "$JSONFILE" ]]; then
            jq -r '.. | objects | .subdomain? // empty' "$JSONFILE" \
              | sed -E "s#^https?://##I; s/^[*.]+//; s/[.]+$//; s/^[[:space:]]+//; s/[[:space:]]+$//" \
              | grep -E '^[a-z0-9]([a-z0-9-]*\.)+[a-z]{2,}$' --color=never \
              | sort -u > "$TMP_ONEFOR" || true
          else
            : > "$TMP_ONEFOR"
          fi
          mv -f "$TMP_ONEFOR" "$ONEFORALL_TXT"
          chmod 644 "$ONEFORALL_TXT"
          echo "oneforall lines: $(wc -l < "$ONEFORALL_TXT")"

          # --- BBOT extraction (short logic, prefer subdomains.txt then JSON) ---

          echo "Running bbot directly (output -> $OUTDIR/bbot-output)"
          mkdir -p "$OUTDIR/bbot-output"
          # run bbot and capture logs (non-fatal)
          bbot -t "${{ matrix.domain }}" -p subdomain-enum -rf passive -ef aggressive -em massdns,asn,wappalyzer -o "$BBOT_OUTDIR" -om subdomains --silent --yes || true
          echo "bbot run logs (err head):"
          head -c 400 "$OUTDIR/bbot.run.err" 2>/dev/null || true

          echo "=== BBOT captured logs (first 200 bytes stderr/stdout) ==="
          head -c 200 "results/${{ matrix.domain }}/bbot.run.err" 2>/dev/null || echo "(no bbot.run.err)"
          head -c 200 "results/${{ matrix.domain }}/bbot.run.log" 2>/dev/null || echo "(no bbot.run.log)"

          echo "=== OUTDIR listing (top 3 levels): ==="
          find "results/${{ matrix.domain }}" -maxdepth 3 -type d -print -exec ls -la {} \; 2>/dev/null || true

          echo "=== Any subdomains.* files (case-ins) and their sizes: ==="
          find "results/${{ matrix.domain }}" -type f \( -iname "subdomains.txt" -o -iname "*subdomains*" -o -iname "*subdomain*" \) -printf "%p %s\n" -quit 2>/dev/null || true

          echo "=== Grep domain-like tokens from any file under OUTDIR (first 40 matches) ==="
          grep -Eoi "([a-z0-9-]+\.)+[a-z]{2,}" "results/${{ matrix.domain }}" -R --no-messages | sed -n "1,40p" || echo "(no domain-like tokens found)" 

          echo "=== Show any JSON files under OUTDIR (first file sample) ==="
          j=$(find "results/${{ matrix.domain }}" -type f -iname "*.json" -print -quit || true)
          if [[ -n "$j" ]]; then
            echo "example json: $j"
            head -n 50 "$j" | sed -n "1,200p"
          else
            echo "no json files found under results/${{ matrix.domain }}"
          fi

          # --- Wait & locate subdomains.txt anywhere under OUTDIR (120s max) ---
          TMP_BBOT="$(mktemp)"
          FOUND=""
          for i in $(seq 1 120); do
            FOUND=$(find "$BBOT_OUTDIR" -type f -iname "subdomains.txt" -print -quit 2>/dev/null || true)
            if [[ -n "$FOUND" ]]; then break; fi
            sleep 1
          done

          # example: normalize and validate an input file -> normalized.tmp
          IN_FILE="$FOUND"

          if [[ -n "$IN_FILE" && -f "$IN_FILE" ]]; then
            sed -E "s#^https?://##I; s/^[*.]+//; s/^[.]+//; s/[.]+$//; s/^[[:space:]]+//; s/[[:space:]]+$//" "$IN_FILE" \
              | sort -u > "$TMP_BBOT"
          else
            : > "$TMP_BBOT"
          fi

          mv -f "$TMP_BBOT" "$BBOT_TXT"
          chmod 644 "$BBOT_TXT"
          echo "bbot lines: $(wc -l < "$BBOT_TXT")"

          # --- External: download and normalize (fixed sed input) ---
          mkdir -p "$OUTDIR"
          REMOTE_URL="https://raw.githubusercontent.com/rix4uni/BugBountyData/refs/heads/main/data/${{ matrix.domain }}.txt"
          if curl -fsSL "$REMOTE_URL" -o "$EXTERNAL_TX"; then
            sed -E "s#^https?://##I; s/^[*.]+//; s/[.]+$//; s/^[[:space:]]+//; s/[[:space:]]+$//" "$EXTERNAL_TX" \
              | sort -u > "$EXTERNAL_FINAL" || true
          else
            : > "$EXTERNAL_FINAL"
          fi
          rm -f "$EXTERNAL_TX"
          echo "external lines: $(wc -l < "$EXTERNAL_FINAL")"

          # --- Combine: only cat files that exist to avoid errors ---
          FILES=(
            "$OUTDIR/subfinder.txt"
            "$OUTDIR/assetfinder.txt"
            "$OUTDIR/amass.txt"
            "$OUTDIR/shrewdeye.txt"
            "$ONEFORALL_TXT"
            "$BBOT_TXT"
            "$OUTDIR/chaos.txt"
            "$EXTERNAL_FINAL"
          )

          # create a temp aggregate file then move atomically
          TMP_ALL="$(mktemp)"
          for f in "${FILES[@]}"; do
            if [[ -f "$f" && -s "$f" ]]; then
              sed -E "s#^https?://##I; s/^[*.]+//; s/[.]+$//; s/^[[:space:]]+//; s/[[:space:]]+$//" "$f"
            fi
          done \
            | inscope --scope .scope \
            | sort -u > "$TMP_ALL" || true

          mv -f "$TMP_ALL" "$TARGET"
          chmod 644 "$TARGET"
          echo "all_subdomains lines: $(wc -l < "$TARGET")"

          # Count all Subdomains
       
      - name: Upload all_subdomains.txt for 
        uses: actions/upload-artifact@v4
        with:
          name: recon-results-${{ matrix.domain }}
          path: results/${{ matrix.domain }}
          retention-days: 1

  commit_results:
    name: Commit All Recon Results
    # This job runs after all parallel scan jobs are finished.
    needs: subdomain_scan
    # 'if: always()' ensures we commit results from successful scans even if some scans failed.
    if: always()
    runs-on: ubuntu-latest
    container:
      # Use the same container to ensure tools like 'anew' are available.
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}
    steps:
      - name: Download all recon artifacts
        uses: actions/download-artifact@v4
        with:
          # Download all artifacts into separate directories under 'temp-results'
          pattern: recon-results-*
          path: temp-results/

      - name: Organize and Push to store-recon
        id: Organize-push
        env:
          STORE_RECON_PAT: ${{ secrets.STORE_RECON_PAT }}
          STORE_REPO_OWNER: ${{ secrets.STORE_OWNER }} # IMPORTANT: Change to the owner of store-recon
          STORE_REPO_NAME: ${{ secrets.STORE_REPO_NAME }}
        run: |
          set -e # Exit immediately if a command fails

          if [ ! -d "temp-results" ] || [ -z "$(ls -A temp-results)" ]; then
            echo "No artifacts found to process. Exiting."
            exit 0
          fi

          echo "Cloning the 'store-recon' repository..."
          CLONE_DIR=$(mktemp -d)
          git clone "https://x-access-token:${STORE_RECON_PAT}@github.com/${STORE_REPO_OWNER}/${STORE_REPO_NAME}.git" "$CLONE_DIR"
          cd "$CLONE_DIR"

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Robust retry loop to handle any remote race conditions on the store-recon repo
          for i in {1..10}; do
            # Always start from the freshest state of the remote branch
            git fetch origin main
            git reset --hard origin/main
            git pull origin main --rebase

            echo "Merging new results into the repository..."

            # Iterate through each downloaded artifact directory (e.g., 'temp-results/recon-results-domain1.com')
            for artifact_path in ${GITHUB_WORKSPACE}/temp-results/*; do
              if [ -d "$artifact_path" ]; then
                source_file="$artifact_path/all_subdomains.txt"
                
                # Check if the primary results file exists in the artifact
                if [ -f "$source_file" ]; then
                  # Extract the domain name by removing the 'recon-results-' prefix from the directory name
                  domain=$(basename "$artifact_path" | sed 's/^recon-results-//')
                  
                  # Define the target directory structure as requested: 'results/domain.com/'
                  target_dir="results/$domain"
                  target_file="$target_dir/all_subdomains.txt"

                  echo "Processing results for: $domain"
                  
                  # Create the target directory (e.g., 'results/domain1.com').
                  # 'mkdir -p' will do nothing if the directory already exists. It will not overwrite.
                  mkdir -p "$target_dir"

                  # This is the key command for your requirement:
                  # It reads the new results and appends only the unique, new lines to the target file.
                  # If the target file doesn't exist, 'anew' creates it.
                  cat "$source_file" | anew -q "$target_file"
                fi
              fi
            done

            git add results/

            # Check for changes before committing to avoid empty commits
            if git diff --staged --quiet; then
              echo "No new unique subdomains found. Repository is already up-to-date."
              exit 0
            fi

            echo "Committing updated results..."
            git commit -m "Daily Recon: Update subdomain results (Run: ${{ github.run_id }})"

            if git push origin main; then
              echo "Successfully pushed results to ${STORE_REPO_NAME} on attempt #$i."
              exit 0 # Success!
            else
              echo "Push failed on attempt #$i. Retrying in 15 seconds..."
              sleep 15
            fi
          done

          echo "ERROR: Could not push results to ${STORE_REPO_NAME} after 10 attempts."
          exit 0

